# -*- coding: utf-8 -*-
"""run_pipeline.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Z4EYoAvYQwoAWmBDIyCdMKCggVaQDkTE
"""

#!/usr/bin/env python3
"""
run_pipeline.py

- Checks the INPUT directory for changes (file-level MD5)
- If any file is new/changed/deleted, runs the cleaning script
- Logs runs to a CSV log file
- Intended to be invoked by cron (or manually)
"""

import os
import sys
import json
import hashlib
import subprocess
from datetime import datetime
import argparse
import logging

# CONFIG
INPUT_DIR = "data/input"
HASH_STORE = "hash_store.json"
RUN_LOG = "logs/pipeline_runs.csv"
CLEANING_SCRIPT = "cleaning.py"   # script to execute

os.makedirs("logs", exist_ok=True)
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

def md5_for_file(path, block_size=65536):
    hasher = hashlib.md5()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(block_size), b""):
            hasher.update(chunk)
    return hasher.hexdigest()

def load_hash_store():
    if os.path.exists(HASH_STORE):
        with open(HASH_STORE, "r", encoding="utf-8") as fh:
            return json.load(fh)
    return {}

def save_hash_store(d):
    with open(HASH_STORE, "w", encoding="utf-8") as fh:
        json.dump(d, fh, indent=2)

def list_input_files(folder):
    return sorted([os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(".jsonl")])

def log_run_to_csv(success: bool, message: str):
    header = "timestamp,success,message\n"
    line = f"{datetime.utcnow().isoformat()},{int(success)},{message}\n"
    if not os.path.exists(RUN_LOG):
        with open(RUN_LOG, "w", encoding="utf-8") as fh:
            fh.write(header)
            fh.write(line)
    else:
        with open(RUN_LOG, "a", encoding="utf-8") as fh:
            fh.write(line)

def run_cleaning_script():
    """Call cleaning.py via subprocess. Capture output and return exit code."""
    cmd = [sys.executable, CLEANING_SCRIPT]  # uses same Python interpreter
    try:
        logging.info("Calling cleaning script: " + " ".join(cmd))
        proc = subprocess.run(cmd, capture_output=True, text=True, check=False)
        logging.info("Cleaning stdout:\n" + proc.stdout)
        if proc.returncode != 0:
            logging.error("Cleaning stderr:\n" + proc.stderr)
        return proc.returncode, proc.stdout + "\n" + proc.stderr
    except Exception as ex:
        logging.exception("Failed to run cleaning script")
        return 1, str(ex)

def main(input_dir=INPUT_DIR):
    changed = False
    messages = []
    stored = load_hash_store()
    current_files = list_input_files(input_dir)

    current_hashes = {}
    for p in current_files:
        h = md5_for_file(p)
        current_hashes[p] = h
        if p not in stored or stored.get(p) != h:
            logging.info(f"Detected change/new file: {p}")
            changed = True

    # detect removed files
    removed = [p for p in stored.keys() if p not in current_hashes]
    if removed:
        logging.info(f"Detected removed files: {removed}")
        changed = True

    if changed:
        # run cleaning
        rc, output = run_cleaning_script()
        if rc == 0:
            logging.info("Cleaning completed successfully.")
            # update store
            save_hash_store(current_hashes)
            log_run_to_csv(True, "Cleaning OK")
        else:
            logging.error("Cleaning failed. Storing run log.")
            log_run_to_csv(False, f"Cleaning failed. rc={rc}. output={output[:200]}")
    else:
        logging.info("No changes detected. Pipeline will not run.")
        log_run_to_csv(True, "No change - skipped")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--input", help="input directory", default=INPUT_DIR)
    args = parser.parse_args()
    main(input_dir=args.input)