# -*- coding: utf-8 -*-
"""cleaning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17hMhYQEYwQT-S57ApyRP0HRrAPVxDinP
"""

#!/usr/bin/env python3
"""
cleaning.py

- Reads all .jsonl files from DATA_INPUT_DIR
- Extracts and normalizes 'data' -> df_data and 'included' -> df_included
- Performs type coercion, text cleaning, duplicate removal, outlier handling for 'confidence'
- Produces before/after stats, saves cleaned CSVs, stats JSON, and writes to SQLite (optional)
- Drops 'body' column from df_data only at the end (per your request)
"""

import os
import json
import hashlib
from datetime import datetime
import pandas as pd
import numpy as np
import sqlite3
import logging
import argparse
from typing import Tuple, Dict, Any

# ----------------------------
# CONFIGURATION (edit as needed)
# ----------------------------
DATA_INPUT_DIR = "data/input"          # directory where your .jsonl files live
OUTPUT_DIR = "data/processed"          # where to write cleaned CSVs and stats
STATS_DIR = "logs"
SQLITE_DB = "data/news_events_cleaned.db"  # optional sqlite db path
DROP_BODY_AT_END = True                # drop 'body' column only at the end
CONFIDENCE_IQR_REMOVE = True           # whether to detect/remove confidence outliers using IQR
CONFIDENCE_LOWER = 0.0                 # allowed min
CONFIDENCE_UPPER = 1.0                 # allowed max

# Create directories if missing
os.makedirs(DATA_INPUT_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(STATS_DIR, exist_ok=True)

# Logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

# ----------------------------
# Helper functions
# ----------------------------
def list_jsonl_files(folder: str):
    return sorted([os.path.join(folder, f) for f in os.listdir(folder) if f.lower().endswith(".jsonl")])

def read_jsonl_file(path: str):
    """Yield parsed json objects, one per line. Uses only builtin json (no jsonlines dep)."""
    with open(path, "r", encoding="utf-8") as fh:
        for line in fh:
            line = line.strip()
            if not line:
                continue
            yield json.loads(line)

def extract_tables_from_files(file_paths):
    """
    Returns two lists: data_records (list of dicts) and included_records (list of dicts)
    """
    data_records = []
    included_records = []
    for p in file_paths:
        logging.info(f"Reading {p}")
        for rec in read_jsonl_file(p):
            # 'data' is typically list
            for item in rec.get("data", []):
                # flatten attributes and relationships into fields we want for df_data
                attributes = item.get("attributes", {}) or {}
                relationships = item.get("relationships", {}) or {}
                # We preserve original structure in case some fields missing
                data_records.append({
                    "event_id": item.get("id"),
                    "summary": attributes.get("summary"),
                    "category": attributes.get("category"),
                    "found_at": attributes.get("found_at"),
                    "confidence": attributes.get("confidence"),
                    "article_sentence": attributes.get("article_sentence"),
                    "location": attributes.get("location"),
                    "award": attributes.get("award"),
                    "effective_date": attributes.get("effective_date"),
                    "product": attributes.get("product"),
                    "company_id": relationships.get("company1", {}).get("data", {}).get("id") if relationships.get("company1") else None,
                    "source_id": relationships.get("most_relevant_source", {}).get("data", {}).get("id") if relationships.get("most_relevant_source") else None,
                    "body": attributes.get("body")  # assume body may exist in attributes
                })
            # 'included' is list of metadata objects
            for item in rec.get("included", []):
                attrs = item.get("attributes", {}) or {}
                flat = {
                    "included_id": item.get("id"),
                    "type": item.get("type"),
                }
                # merge attributes keys (domain, company_name, ticker, author, body, image_url, url, published_at, title, ...)
                for k, v in attrs.items():
                    flat[k] = v
                included_records.append(flat)

    return data_records, included_records

def basic_profile_df(df: pd.DataFrame) -> pd.DataFrame:
    """Return a simple profile DataFrame per column."""
    rows = []
    for c in df.columns:
        col = df[c]
        dtype = str(col.dtype)
        num_missing = int(col.isnull().sum())
        pct_missing = float((num_missing / len(df)) * 100) if len(df) else 0.0
        num_unique = int(col.nunique(dropna=True))
        # numeric metrics only when numeric
        if pd.api.types.is_numeric_dtype(col):
            minv = float(col.min()) if not col.isnull().all() else None
            maxv = float(col.max()) if not col.isnull().all() else None
            meanv = float(col.mean()) if not col.isnull().all() else None
            medi = float(col.median()) if not col.isnull().all() else None
        else:
            minv = maxv = meanv = medi = None
        rows.append({
            "column": c, "dtype": dtype,
            "num_missing": num_missing, "pct_missing": pct_missing,
            "num_unique": num_unique,
            "min": minv, "max": maxv, "mean": meanv, "median": medi
        })
    return pd.DataFrame(rows)

def clean_text_columns(df: pd.DataFrame):
    """Remove newlines, collapse whitespace, convert 'nan' strings back to NaN."""
    obj_cols = df.select_dtypes(include=["object"]).columns
    for c in obj_cols:
        df[c] = df[c].astype(str).replace(r'[\r\n]+', ' ', regex=True)
        df[c] = df[c].replace(r'\s+', ' ', regex=True).str.strip()
        df[c] = df[c].replace({'nan': None, 'None': None})
    return df

def enforce_types(df: pd.DataFrame):
    """Coerce columns to desired types where possible."""
    # datetimes
    if "found_at" in df.columns:
        df["found_at"] = pd.to_datetime(df["found_at"], errors="coerce")
    if "effective_date" in df.columns:
        df["effective_date"] = pd.to_datetime(df["effective_date"], errors="coerce")
    # confidence numeric
    if "confidence" in df.columns:
        df["confidence"] = pd.to_numeric(df["confidence"], errors="coerce")
    return df

def detect_iqr_bounds(ser: pd.Series) -> Dict[str, float]:
    """Return IQR bounds for series (assumes numeric)."""
    q1 = ser.quantile(0.25)
    q3 = ser.quantile(0.75)
    iqr = q3 - q1
    lower = q1 - 1.5 * iqr
    upper = q3 + 1.5 * iqr
    return {"q1": float(q1), "q3": float(q3), "lower": float(lower), "upper": float(upper)}

def remove_outliers_iqr(df: pd.DataFrame, col: str) -> Tuple[pd.DataFrame, int, Dict[str, float]]:
    """Remove rows where col is outside IQR bounds. Return new df, count removed, and the bounds."""
    ser = df[col].dropna()
    if ser.empty:
        return df, 0, {}
    bounds = detect_iqr_bounds(ser)
    before = len(df)
    df_clean = df[(df[col].isna()) | ((df[col] >= bounds["lower"]) & (df[col] <= bounds["upper"]))]
    removed = before - len(df_clean)
    return df_clean, removed, bounds

def drop_heavy_or_allnan_columns(df: pd.DataFrame, threshold_pct=95.0) -> Tuple[pd.DataFrame, list]:
    """Drop columns that are completely empty or have > threshold_pct missing."""
    cols = df.columns[df.isnull().mean() * 100 > threshold_pct].tolist()
    # Also drop columns that are entirely null
    all_null_cols = df.columns[df.isnull().all()].tolist()
    to_drop = sorted(set(cols + all_null_cols))
    if to_drop:
        df.drop(columns=to_drop, inplace=True, errors=True)
    return df, to_drop

def save_stats_json(stats: Dict[str, Any], out_path: str):
    with open(out_path, "w", encoding="utf-8") as fh:
        json.dump(stats, fh, default=str, indent=2)

# ----------------------------
# Main cleaning workflow
# ----------------------------
def run_cleaning(input_dir: str = DATA_INPUT_DIR, output_dir: str = OUTPUT_DIR, sqlite_db: str = SQLITE_DB) -> Dict[str, Any]:
    files = list_jsonl_files(input_dir)
    if not files:
        logging.info("No .jsonl files found in input folder. Exiting.")
        return {"status": "no_files"}

    logging.info(f"Found {len(files)} .jsonl files. Extracting records...")
    data_records, included_records = extract_tables_from_files(files)

    # Convert to DataFrames
    df_data = pd.DataFrame(data_records)
    df_included = pd.DataFrame(included_records)

    # Capture before profiles
    before_profile_data = basic_profile_df(df_data)
    before_profile_included = basic_profile_df(df_included)
    before_counts = {"df_data_rows": len(df_data), "df_included_rows": len(df_included)}

    # Cleaning steps
    # 1) Basic text cleaning
    df_data = clean_text_columns(df_data)
    df_included = clean_text_columns(df_included)

    # 2) Type enforcement
    df_data = enforce_types(df_data)
    df_included = enforce_types(df_included)

    # 3) Remove duplicate event rows (by event_id) and included (by included_id)
    if "event_id" in df_data.columns:
        dups = int(df_data.duplicated(subset=["event_id"]).sum())
        logging.info(f"Found {dups} duplicate event_id rows; dropping duplicates.")
        df_data = df_data.drop_duplicates(subset=["event_id"])
    if "included_id" in df_included.columns:
        dups_inc = int(df_included.duplicated(subset=["included_id"]).sum())
        logging.info(f"Found {dups_inc} duplicate included_id rows; dropping duplicates.")
        df_included = df_included.drop_duplicates(subset=["included_id"])

    # 4) Confidence validation and outlier removal
    outlier_info = {}
    outliers_removed = 0
    if "confidence" in df_data.columns:
        # Bound confidence to allowed numeric range first
        before_conf_invalid = int(((df_data["confidence"] < CONFIDENCE_LOWER) | (df_data["confidence"] > CONFIDENCE_UPPER)).sum())
        if before_conf_invalid > 0:
            logging.info(f"Found {before_conf_invalid} confidence values outside [{CONFIDENCE_LOWER},{CONFIDENCE_UPPER}]; setting to NaN")
            df_data.loc[(df_data["confidence"] < CONFIDENCE_LOWER) | (df_data["confidence"] > CONFIDENCE_UPPER), "confidence"] = np.nan

        # Fill or handle NaNs: we won't fill yet; instead use IQR-based outlier detection if enabled
        if CONFIDENCE_IQR_REMOVE:
            df_data, removed_count, bounds = remove_outliers_iqr(df_data, "confidence")
            outlier_info = {"removed": int(removed_count), "bounds": bounds}
            outliers_removed = int(removed_count)

    # 5) Derived columns
    if "summary" in df_data.columns:
        df_data["summary_length"] = df_data["summary"].astype(str).apply(len)
    if "article_sentence" in df_data.columns:
        df_data["article_sentence_length"] = df_data["article_sentence"].astype(str).apply(len)
    if "found_at" in df_data.columns:
        # Ensure found_at is timezone-naive for storage comparison (optional)
        df_data["found_at"] = pd.to_datetime(df_data["found_at"], errors="coerce")
        df_data["found_date"] = df_data["found_at"].dt.date

    # 6) Drop columns with >95% missing
    df_data, dropped_data_cols = drop_heavy_or_allnan_columns(df_data, threshold_pct=95.0)
    df_included, dropped_included_cols = drop_heavy_or_allnan_columns(df_included, threshold_pct=95.0)

    # 7) Final drop: drop 'body' in df_data only at the end (per request)
    if DROP_BODY_AT_END and "body" in df_data.columns:
        df_data.drop(columns=["body"], inplace=True, errors=True)
        logging.info("Dropped 'body' from df_data (per configuration)")

    # 8) Save cleaned outputs
    timestamp = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    cleaned_data_csv = os.path.join(output_dir, f"cleaned_df_data_{timestamp}.csv")
    cleaned_included_csv = os.path.join(output_dir, f"cleaned_df_included_{timestamp}.csv")
    df_data.to_csv(cleaned_data_csv, index=False, encoding="utf-8")
    df_included.to_csv(cleaned_included_csv, index=False, encoding="utf-8")
    logging.info(f"Wrote cleaned files: {cleaned_data_csv}, {cleaned_included_csv}")

    # 9) Save to sqlite (optional)
    try:
        conn = sqlite3.connect(sqlite_db)
        df_data.to_sql("news_events_cleaned", conn, if_exists="replace", index=False)
        df_included.to_sql("included_cleaned", conn, if_exists="replace", index=False)
        conn.close()
        logging.info(f"Wrote cleaned tables to sqlite: {sqlite_db}")
    except Exception as ex:
        logging.warning(f"Could not write to sqlite ({sqlite_db}): {ex}")

    # 10) After profiles
    after_profile_data = basic_profile_df(df_data)
    after_profile_included = basic_profile_df(df_included)
    after_counts = {"df_data_rows": len(df_data), "df_included_rows": len(df_included)}

    # 11) Save stats JSON
    stats = {
        "run_timestamp": timestamp,
        "before_counts": before_counts,
        "after_counts": after_counts,
        "outlier_info": outlier_info,
        "dropped_data_columns": dropped_data_cols,
        "dropped_included_columns": dropped_included_cols,
        "before_profile_data": before_profile_data.to_dict(orient="records"),
        "after_profile_data": after_profile_data.to_dict(orient="records"),
        "before_profile_included": before_profile_included.to_dict(orient="records"),
        "after_profile_included": after_profile_included.to_dict(orient="records")
    }
    stats_path = os.path.join(STATS_DIR, f"dq_stats_{timestamp}.json")
    save_stats_json(stats, stats_path)
    logging.info(f"Saved stats to {stats_path}")

    return {
        "status": "ok",
        "cleaned_data_csv": cleaned_data_csv,
        "cleaned_included_csv": cleaned_included_csv,
        "stats_path": stats_path,
        "rows_before": before_counts,
        "rows_after": after_counts,
        "outliers_removed": outliers_removed
    }

# ----------------------------
# CLI entrypoint
# ----------------------------
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="News events cleaning script")
    parser.add_argument("--input", type=str, default=DATA_INPUT_DIR, help="Input folder for .jsonl files")
    parser.add_argument("--output", type=str, default=OUTPUT_DIR, help="Output folder for cleaned CSVs")
    parser.add_argument("--sqlite", type=str, default=SQLITE_DB, help="SQLite DB path to write cleaned tables")
    args = parser.parse_args()

    # override defaults from CLI
    DATA_INPUT_DIR = args.input
    OUTPUT_DIR = args.output
    SQLITE_DB = args.sqlite

    result = run_cleaning(DATA_INPUT_DIR, OUTPUT_DIR, SQLITE_DB)
    logging.info("Finished cleaning run.")
    print(json.dumps(result, indent=2, default=str))